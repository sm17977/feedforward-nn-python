{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'pp-2021.csv'\n",
    "data = pandas.read_csv(path, sep=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(data.columns[[0, 2, 7, 8, 9, 10, 12, 13, 14, 15]], axis=1)\n",
    "data.columns = [\"Price\", \"Postcode\", \"PropertyType\", \"OldNew\", \"Duration\", \"City\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove rows that describe properties that are not typically Houses\n",
    "data = data[~data[\"PropertyType\"].isin([\"F\", \"O\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For cities that appear in the data less than 600 times, join into \"Other\" category\n",
    "city_stats = data.groupby('City')['City'].agg('count').sort_values(ascending=False)\n",
    "city_stats_less_than_600 = city_stats[city_stats <= 600]\n",
    "data[\"City\"] = data[\"City\"].apply(lambda x: 'Other' if x in city_stats_less_than_600 else x)\n",
    "# Print city count distribution\n",
    "city_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove rows with prices lower than £40,000 and higher than £5,000,000\n",
    "data = data[data[\"Price\"] > 40000]\n",
    "data = data[data[\"Price\"] < 5000000]\n",
    "data.Price.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert PropertyType and Duration from string to int\n",
    "data['PropertyType'] = [ord(x) - 64 for x in data.PropertyType]\n",
    "data['Duration'] = [ord(x) - 64 for x in data.Duration]\n",
    "# Convert OldNew from Y/N to 1/0\n",
    "data['OldNew'] = data['OldNew'].map({'Y': 1, 'N': 0})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import/Install pgeocode lib to get longitude and latitude from a postcode string\n",
    "import sys\n",
    "try:\n",
    "    import pgeocode\n",
    "except ImportError as e:\n",
    "    !conda install --channel conda-forge --yes --prefix {sys.prefix} pgeocode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove all rows with NaN\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format postcode data in main df so that it matches the format in the pgeocode lib\n",
    "codes = [str(x) for x in data.Postcode]\n",
    "codes = [x[0:x.index(\" \")] for x in codes if \" \" in x]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get longitude and latitude values for every valid postcode\n",
    "nomi = pgeocode.Nominatim(\"GB\")\n",
    "post_code_info = nomi.query_postal_code(codes)\n",
    "latitude = post_code_info.latitude\n",
    "longitude = post_code_info.longitude\n",
    "post_code_info"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Store long and lat in main df\n",
    "data['Longitude'] = longitude\n",
    "data['Latitude'] = latitude\n",
    "\n",
    "# Drop Postcode col now we have location data in long + lat\n",
    "data.drop(['Postcode'], inplace=True, axis=1)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Convert each city into a column using one-hot encoding, remove old City col\n",
    "dummies = pandas.get_dummies(data.City)\n",
    "dummies.drop(('Other'), inplace=True, axis=1)\n",
    "data = pd.concat([data, dummies], axis=1)\n",
    "data.drop(['City'], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Normalize Price between -1 and 1\n",
    "min_params = data['Price'].min()\n",
    "max_params = data['Price'].max()\n",
    "data['Price'] = 2 * ((data['Price'] - min_params) / (max_params - min_params)) -1\n",
    "\n",
    "# Normalize lat and longs between~ -1 and 1\n",
    "data[['Longitude', 'Latitude']] = (data[['Longitude', 'Latitude']] / 100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Training, Validation, Test split = 80%, 10%, 10%\n",
    "train, val, test = np.split(data.sample(frac=1, random_state=54), [int(.8 * len(data)), int(.9 * len(data))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# y = column vector, true price value (labels) (len(data[0],)\n",
    "y_train, y_val, y_test = train.Price, val.Price, test.Price\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# X = matrix, excludes price feature\n",
    "train.drop(['Price'], axis=1, inplace=True)\n",
    "X_train = train.to_numpy()\n",
    "val.drop(['Price'], axis=1, inplace=True)\n",
    "X_val = val.to_numpy()\n",
    "test.drop(['Price'], axis=1, inplace=True)\n",
    "X_test = test.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Shuffle order of indexes for training data\n",
    "np.set_printoptions(threshold=0)\n",
    "keys = np.array(range(len(y_train)))\n",
    "np.random.shuffle(keys)\n",
    "X_train = X_train[keys]\n",
    "y_train = y_train[keys]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dense_Layer:\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0., weight_regularizer_l2=0.,\n",
    "                 bias_regularizer_l1=0., bias_regularizer_l2=0.):\n",
    "\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Dropout_Layer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Linear_Activation:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class ReLU_Activation:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y, layers, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y, layers)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        return data_loss, self.regularization_loss(layers)\n",
    "\n",
    "    def calculate_accumulated(self, include_regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "\n",
    "        return data_loss, self.regularization_loss(layers)\n",
    "\n",
    "    def regularization_loss(self, layers_list):\n",
    "\n",
    "        regularization_loss = 0\n",
    "\n",
    "        for layer in layers_list:\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MSE_Loss(Loss):\n",
    "    def forward(self, y, y_hat, layers):\n",
    "        sample_losses = np.mean((y_hat - y)**2, axis=-1)\n",
    "        return sample_losses\n",
    "    def backward(self, dvalues, y_hat):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "        for batch in y_hat:\n",
    "            self.dinputs = (-2 * (batch - dvalues)) / samples\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SGD_Optimzer:\n",
    "    def __init__(self, learning_rate = 0.01, decay=1e-6, momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def calculate(self, predictions, y):\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_accumulated(self):\n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "        # Reset variables for accumulated accuracy\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y)\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dense1 = Dense_Layer(X_train.shape[1], 128, weight_regularizer_l1=1e-4, bias_regularizer_l1=1e-4)\n",
    "activation1 = ReLU_Activation()\n",
    "dropout1 = Dropout_Layer(0.1)\n",
    "dense2 = Dense_Layer(128, 128)\n",
    "activation2 = ReLU_Activation()\n",
    "dropout2 = Dropout_Layer(0.1)\n",
    "dense3 = Dense_Layer(128, 1)\n",
    "activation3 = Linear_Activation()\n",
    "loss_function = MSE_Loss()\n",
    "#optimzer = SGD_Optimzer(learning_rate=0.01, decay=4e-5)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.01, decay=1e-3)\n",
    "batch_accuracy = Accuracy()\n",
    "\n",
    "\n",
    "\n",
    "layers = [dense1, dense2, dense3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_steps = X_train.shape[0] // batch_size\n",
    "val_steps = X_val.shape[0] //batch_size\n",
    "\n",
    "if train_steps * batch_size < X_train.shape[0]:\n",
    "    train_steps += 1\n",
    "\n",
    "if val_steps * batch_size < X_val.shape[0]:\n",
    "    val_steps += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Begin training model\n",
    "for epoch in range(10):\n",
    "    print(f'epoch: {epoch}')\n",
    "    np.random.shuffle(keys)\n",
    "    X_train = X_train[keys]\n",
    "    y_train = y_train[keys]\n",
    "    loss_function.new_pass()\n",
    "    batch_accuracy.new_pass()\n",
    "\n",
    "    # Train model in mini batches\n",
    "    for step in range(train_steps):\n",
    "        batch_X = X_train[step * batch_size:(step+1)*batch_size]\n",
    "        batch_y = y_train[step * batch_size:(step+1)*batch_size]\n",
    "\n",
    "        # Forward Pass\n",
    "        dense1.forward(batch_X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dropout1.forward(activation1.output)\n",
    "        dense2.forward(dropout1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "        dropout2.forward(activation2.output)\n",
    "        dense3.forward(dropout2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        # Calculate Loss of current batch\n",
    "        data_loss, reg_loss = loss_function.calculate(activation3.output, batch_y, layers, include_regularization=True)\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        # Predicted Prices of current batch\n",
    "        predictions = activation3.output\n",
    "\n",
    "        # Backward Pass\n",
    "        loss_function.backward(activation3.output, batch_y)\n",
    "        activation3.backward(loss_function.dinputs)\n",
    "        dense3.backward(activation3.dinputs)\n",
    "        dropout2.backward(dense3.dinputs)\n",
    "        activation2.backward(dropout2.dinputs)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        dropout1.backward(dense2.dinputs)\n",
    "        activation1.backward(dropout1.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        # Print loss every 1000 batches\n",
    "        if not step % 1000:\n",
    "            print(f'batch: {step} out of {train_steps}, loss: {loss:.3f}')\n",
    "\n",
    "    # Calculate mean Loss after epoch\n",
    "    epoch_data_loss, epoch_reg_loss = \\\n",
    "        loss_function.calculate_accumulated(include_regularization=True)\n",
    "    epoch_loss = epoch_data_loss + epoch_reg_loss\n",
    "\n",
    "    # Print training progress after epoch\n",
    "    print(f'training, ' +\n",
    "    f'loss: {epoch_loss:.3f} (' +\n",
    "    f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "    f'reg_loss: {epoch_reg_loss:.3f}, ' +\n",
    "    f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Reset mean Loss before using validation set\n",
    "    loss_function.new_pass()\n",
    "    batch_accuracy.new_pass()\n",
    "\n",
    "    # Pass through validation set for evaluation during training\n",
    "    for step in range(val_steps):\n",
    "\n",
    "        batch_X = X_val[step * batch_size:(step+1)*batch_size]\n",
    "        batch_y = y_val[step * batch_size:(step+1)*batch_size]\n",
    "\n",
    "        dense1.forward(batch_X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        loss_function.calculate(activation3.output, batch_y, layers)\n",
    "\n",
    "        predictions = activation3.output\n",
    "\n",
    "    # Calculate mean Loss for validation set on current model parameters\n",
    "    val_loss = loss_function.calculate_accumulated()\n",
    "\n",
    "    # Print mean validation loss per epoch\n",
    "    print(f'validation, ' +\n",
    "          f'loss: {val_loss:.3f}, ')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (feedforward-nn-python)",
   "language": "python",
   "name": "pycharm-41e5e3a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}