{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "matplotlib.use(\"TkAgg\")\n",
    "pandas.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'pp-2021.csv'\n",
    "data = pandas.read_csv(path, sep=',', skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = data.drop(data.columns[[0, 2, 7, 8, 9, 10, 12, 13, 14, 15]], axis=1)\n",
    "data.columns = [\"Price\", \"Postcode\", \"PropertyType\", \"OldNew\", \"Duration\", \"City\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove rows that describe properties that are not typically Houses\n",
    "data = data[~data[\"PropertyType\"].isin([\"F\", \"O\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "outputs": [
    {
     "data": {
      "text/plain": "City\nLONDON           26311\nMANCHESTER       13315\nBRISTOL          12349\nNOTTINGHAM       12239\nBIRMINGHAM       11630\n                 ...  \nLYNMOUTH             3\nRHOSGOCH             2\nKELSO                1\nLLANSANFFRAID        1\nNEWCASTLETON         1\nName: City, Length: 1147, dtype: int64"
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For cities that appear in the data less than 600 times, join into \"Other\" category\n",
    "city_stats = data.groupby('City')['City'].agg('count').sort_values(ascending=False)\n",
    "city_stats_less_than_600 = city_stats[city_stats <= 600]\n",
    "data[\"City\"] = data[\"City\"].apply(lambda x: 'Other' if x in city_stats_less_than_600 else x)\n",
    "# Print city count distribution\n",
    "city_stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "outputs": [
    {
     "data": {
      "text/plain": "count    8.658430e+05\nmean     3.432514e+05\nstd      2.911646e+05\nmin      4.010000e+04\n25%      1.785000e+05\n50%      2.729950e+05\n75%      4.125000e+05\nmax      4.999999e+06\nName: Price, dtype: float64"
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with prices lower than £40,000 and higher than £5,000,000\n",
    "data = data[data[\"Price\"] > 40000]\n",
    "data = data[data[\"Price\"] < 5000000]\n",
    "data.Price.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "outputs": [],
   "source": [
    "# Convert PropertyType and Duration from string to int\n",
    "data['PropertyType'] = [ord(x) - 64 for x in data.PropertyType]\n",
    "data['Duration'] = [ord(x) - 64 for x in data.Duration]\n",
    "# Convert OldNew from Y/N to 1/0\n",
    "data['OldNew'] = data['OldNew'].map({'Y': 1, 'N': 0})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "outputs": [],
   "source": [
    "# Import/Install pgeocode lib to get longitude and latitude from a postcode string\n",
    "import sys\n",
    "try:\n",
    "    import pgeocode\n",
    "except ImportError as e:\n",
    "    !conda install --channel conda-forge --yes --prefix {sys.prefix} pgeocode"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "outputs": [
    {
     "data": {
      "text/plain": "         Price  Postcode  PropertyType  OldNew  Duration             City\n0       137000   B98 7BE            20       0         6         REDDITCH\n1       337000   B61 8NJ            19       0         6       BROMSGROVE\n2       178500   HR2 7RU            19       0         6         HEREFORD\n3       178000   B97 6NJ            19       0         6         REDDITCH\n4       212500   B96 6AU            19       0         6         REDDITCH\n...        ...       ...           ...     ...       ...              ...\n865627  155000  WS15 2AU            20       0         6          RUGELEY\n865628  167200   B77 2JF            20       0         6         TAMWORTH\n865629   90000  DE14 3PH            20       0         6  BURTON-ON-TRENT\n865630  255000  WV10 7TU             4       0         6    WOLVERHAMPTON\n865631  249950   WV8 1AL             4       0         6    WOLVERHAMPTON\n\n[865632 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Price</th>\n      <th>Postcode</th>\n      <th>PropertyType</th>\n      <th>OldNew</th>\n      <th>Duration</th>\n      <th>City</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>137000</td>\n      <td>B98 7BE</td>\n      <td>20</td>\n      <td>0</td>\n      <td>6</td>\n      <td>REDDITCH</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>337000</td>\n      <td>B61 8NJ</td>\n      <td>19</td>\n      <td>0</td>\n      <td>6</td>\n      <td>BROMSGROVE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>178500</td>\n      <td>HR2 7RU</td>\n      <td>19</td>\n      <td>0</td>\n      <td>6</td>\n      <td>HEREFORD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>178000</td>\n      <td>B97 6NJ</td>\n      <td>19</td>\n      <td>0</td>\n      <td>6</td>\n      <td>REDDITCH</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>212500</td>\n      <td>B96 6AU</td>\n      <td>19</td>\n      <td>0</td>\n      <td>6</td>\n      <td>REDDITCH</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>865627</th>\n      <td>155000</td>\n      <td>WS15 2AU</td>\n      <td>20</td>\n      <td>0</td>\n      <td>6</td>\n      <td>RUGELEY</td>\n    </tr>\n    <tr>\n      <th>865628</th>\n      <td>167200</td>\n      <td>B77 2JF</td>\n      <td>20</td>\n      <td>0</td>\n      <td>6</td>\n      <td>TAMWORTH</td>\n    </tr>\n    <tr>\n      <th>865629</th>\n      <td>90000</td>\n      <td>DE14 3PH</td>\n      <td>20</td>\n      <td>0</td>\n      <td>6</td>\n      <td>BURTON-ON-TRENT</td>\n    </tr>\n    <tr>\n      <th>865630</th>\n      <td>255000</td>\n      <td>WV10 7TU</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n      <td>WOLVERHAMPTON</td>\n    </tr>\n    <tr>\n      <th>865631</th>\n      <td>249950</td>\n      <td>WV8 1AL</td>\n      <td>4</td>\n      <td>0</td>\n      <td>6</td>\n      <td>WOLVERHAMPTON</td>\n    </tr>\n  </tbody>\n</table>\n<p>865632 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all rows with NaN\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format postcode data in main df so that it matches the format in the pgeocode lib\n",
    "codes = [str(x) for x in data.Postcode]\n",
    "codes = [x[0:x.index(\" \")] for x in codes if \" \" in x]\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "outputs": [
    {
     "data": {
      "text/plain": "count    864151.000000\nmean         -1.391892\nstd           1.332416\nmin          -6.311400\n25%          -2.277712\n50%          -1.432300\n75%          -0.353920\nmax           1.721625\nName: longitude, dtype: float64"
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get longitude and latitude values for every valid postcode\n",
    "nomi = pgeocode.Nominatim(\"GB\")\n",
    "post_code_info = nomi.query_postal_code(codes)\n",
    "latitude = post_code_info.latitude\n",
    "longitude = post_code_info.longitude\n",
    "post_code_info.longitude.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "outputs": [],
   "source": [
    "# Store long and lat in main df\n",
    "data['Longitude'] = longitude\n",
    "data['Latitude'] = latitude\n",
    "\n",
    "# Drop Postcode col now we have location data in long + lat\n",
    "data.drop(['Postcode'], inplace=True, axis=1)\n",
    "\n",
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "outputs": [],
   "source": [
    "# Convert each city into a column using one-hot encoding, remove old City col\n",
    "dummies = pandas.get_dummies(data.City)\n",
    "dummies.drop(('Other'), inplace=True, axis=1)\n",
    "data = pd.concat([data, dummies], axis=1)\n",
    "data.drop(['City'], inplace=True, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.0, 1.0)"
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize Price between -1 and 1\n",
    "#min_params = data['Price'].min()\n",
    "#max_params = data['Price'].max()\n",
    "#data['Price'] =  2 * ((data['Price'] - min_params) / (max_params - min_params)) - 1\n",
    "data['Price'] =  data['Price'] / 1000000\n",
    "\n",
    "# Standardization\n",
    "#data['Price'] = (data['Price'] - data['Price'].mean()) / data['Price'].std()\n",
    "\n",
    "\n",
    "min_params = data['PropertyType'].min()\n",
    "max_params = data['PropertyType'].max()\n",
    "data['PropertyType'] =  ((data['PropertyType'] - min_params) / (max_params - min_params))\n",
    "\n",
    "\n",
    "min_params = data['Duration'].min()\n",
    "max_params = data['Duration'].max()\n",
    "data['Duration'] =  ((data['Duration'] - min_params) / (max_params - min_params))\n",
    "\n",
    "\n",
    "min_params = data['Longitude'].min()\n",
    "max_params = data['Longitude'].max()\n",
    "data['Longitude'] =  ((data['Longitude'] - min_params) / (max_params - min_params))\n",
    "\n",
    "# min_params = data['Latitude'].min()\n",
    "# max_params = data['Latitude'].max()\n",
    "# data['Latitude'] =  ((data['Latitude'] - min_params) / (max_params - min_params))\n",
    "data['Latitude'] = data['Latitude'] / 100\n",
    "\n",
    "min_params, max_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "outputs": [
    {
     "data": {
      "text/plain": "118"
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns.get_loc(\"DURHAM\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "outputs": [],
   "source": [
    "# Training, Validation, Test split = 80%, 10%, 10%\n",
    "train, val, test = np.split(data.sample(frac=1, random_state=54), [int(.8 * len(data)), int(.9 * len(data))])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "outputs": [],
   "source": [
    "# y = column vector, true price value (labels) (len(data[0],)\n",
    "y_train, y_val, y_test = train.Price, val.Price, test.Price\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "outputs": [],
   "source": [
    "# X = matrix, excludes price feature\n",
    "train.drop(['Price'], axis=1, inplace=True)\n",
    "X_train = train.to_numpy()\n",
    "val.drop(['Price'], axis=1, inplace=True)\n",
    "X_val = val.to_numpy()\n",
    "test.drop(['Price'], axis=1, inplace=True)\n",
    "X_test = test.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "outputs": [],
   "source": [
    "# Shuffle order of indexes for training data\n",
    "np.set_printoptions(threshold=0)\n",
    "keys = np.array(range(len(y_train)))\n",
    "np.random.shuffle(keys)\n",
    "X_train = X_train[keys]\n",
    "y_train = y_train[keys]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dense_Layer:\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0., weight_regularizer_l2=0.,\n",
    "                 bias_regularizer_l1=0., bias_regularizer_l2=0.):\n",
    "\n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        self.inputs = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "outputs": [],
   "source": [
    "class Dropout_Layer:\n",
    "    def __init__(self, rate):\n",
    "        self.rate = 1 - rate\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        self.output = inputs * self.binary_mask\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "outputs": [],
   "source": [
    "class Linear_Activation:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "outputs": [],
   "source": [
    "class ReLU_Activation:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y, layers, include_regularization=False):\n",
    "        sample_losses = self.forward(output, y, layers)\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "\n",
    "        self.accumulated_sum += np.sum(sample_losses)\n",
    "        self.accumulated_count += len(sample_losses)\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "        return data_loss, self.regularization_loss(layers)\n",
    "\n",
    "    def calculate_accumulated(self, include_regularization=False):\n",
    "        data_loss = self.accumulated_sum / self.accumulated_count\n",
    "\n",
    "        if not include_regularization:\n",
    "            return data_loss\n",
    "\n",
    "\n",
    "        return data_loss, self.regularization_loss(layers)\n",
    "\n",
    "    def regularization_loss(self, layers_list):\n",
    "\n",
    "        regularization_loss = 0\n",
    "\n",
    "        for layer in layers_list:\n",
    "            if layer.weight_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "\n",
    "            if layer.weight_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
    "\n",
    "            if layer.bias_regularizer_l1 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "\n",
    "            if layer.bias_regularizer_l2 > 0:\n",
    "                regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
    "\n",
    "        return regularization_loss\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "outputs": [],
   "source": [
    "class MSE_Loss(Loss):\n",
    "    def forward(self, y, y_hat, layers):\n",
    "        sample_losses = np.mean((y_hat - y)**2, axis=-1)\n",
    "        return sample_losses\n",
    "    def backward(self, dvalues, y_hat):\n",
    "\n",
    "        samples = len(dvalues)\n",
    "        for batch in y_hat:\n",
    "            self.dinputs = (-2 * (batch - dvalues)) / samples\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "outputs": [],
   "source": [
    "class SGD_Optimzer:\n",
    "    def __init__(self, learning_rate = 0.01, decay=1e-6, momentum=0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1 / (1 + self.decay * self.iterations))\n",
    "    def update_params(self, layer):\n",
    "\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.learning_rate * layer.dweights\n",
    "            bias_updates = -self.learning_rate * layer.dbiases\n",
    "\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "outputs": [],
   "source": [
    "class Optimizer_Adam:\n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "outputs": [],
   "source": [
    "class Accuracy:\n",
    "    def __init__(self):\n",
    "        self.precision = None\n",
    "\n",
    "    def init(self, y, reinit=False):\n",
    "        if self.precision is None or reinit:\n",
    "            self.precision = np.std(y) / 250\n",
    "\n",
    "    def calculate(self, predictions, y):\n",
    "        # Get comparison results\n",
    "        comparisons = self.compare(predictions, y)\n",
    "        # Calculate an accuracy\n",
    "        accuracy = np.mean(comparisons)\n",
    "        # Add accumulated sum of matching values and sample count\n",
    "        self.accumulated_sum += np.sum(comparisons)\n",
    "        self.accumulated_count += len(comparisons)\n",
    "        # Return accuracy\n",
    "        return accuracy\n",
    "\n",
    "    def calculate_accumulated(self):\n",
    "        # Calculate an accuracy\n",
    "        accuracy = self.accumulated_sum / self.accumulated_count\n",
    "        # Return the data and regularization losses\n",
    "        return accuracy\n",
    "        # Reset variables for accumulated accuracy\n",
    "\n",
    "    def compare(self, predictions, y):\n",
    "        return np.absolute(predictions - y)\n",
    "\n",
    "    def new_pass(self):\n",
    "        self.accumulated_sum = 0\n",
    "        self.accumulated_count = 0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "outputs": [],
   "source": [
    "dense1 = Dense_Layer(X_train.shape[1], 128, weight_regularizer_l1=1e-4, bias_regularizer_l1=1e-4)\n",
    "activation1 = ReLU_Activation()\n",
    "dropout1 = Dropout_Layer(0.1)\n",
    "dense2 = Dense_Layer(128, 128)\n",
    "activation2 = ReLU_Activation()\n",
    "dropout2 = Dropout_Layer(0.1)\n",
    "dense3 = Dense_Layer(128, 1)\n",
    "activation3 = Linear_Activation()\n",
    "loss_function = MSE_Loss()\n",
    "#optimizer = SGD_Optimzer(learning_rate=0.01, decay=4e-5)\n",
    "optimizer = Optimizer_Adam(learning_rate=0.001, decay=1e-3)\n",
    "accuracy = Accuracy()\n",
    "\n",
    "\n",
    "\n",
    "layers = [dense1, dense2, dense3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_steps = X_train.shape[0] // batch_size\n",
    "val_steps = X_val.shape[0] // batch_size\n",
    "test_steps = X_val.shape[0] // batch_size\n",
    "\n",
    "if train_steps * batch_size < X_train.shape[0]:\n",
    "    train_steps += 1\n",
    "\n",
    "if val_steps * batch_size < X_val.shape[0]:\n",
    "    val_steps += 1\n",
    "\n",
    "if test_steps * batch_size < X_test.shape[0]:\n",
    "    test_steps += 1\n",
    "\n",
    "pairs = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "batch: 0 out of 5401, loss: 0.731\n",
      "batch: 1000 out of 5401, loss: 0.119\n",
      "batch: 2000 out of 5401, loss: 0.101\n",
      "batch: 3000 out of 5401, loss: 0.057\n",
      "batch: 4000 out of 5401, loss: 0.030\n",
      "batch: 5000 out of 5401, loss: 0.121\n",
      "training, acc: 22.972496901583636, loss: 0.089 (data_loss: 0.087, reg_loss: 0.002, lr: 0.00015625\n",
      "validation, accuracy: 21.65069198494838, loss: 0.088, \n",
      "epoch: 1\n",
      "batch: 0 out of 5401, loss: 0.198\n",
      "batch: 1000 out of 5401, loss: 0.222\n",
      "batch: 2000 out of 5401, loss: 0.082\n",
      "batch: 3000 out of 5401, loss: 0.057\n",
      "batch: 4000 out of 5401, loss: 0.066\n",
      "batch: 5000 out of 5401, loss: 0.066\n",
      "training, acc: 22.987105904592198, loss: 0.087 (data_loss: 0.086, reg_loss: 0.001, lr: 8.473858147614608e-05\n",
      "validation, accuracy: 22.34172769650363, loss: 0.086, \n",
      "epoch: 2\n",
      "batch: 0 out of 5401, loss: 0.039\n",
      "batch: 1000 out of 5401, loss: 0.286\n",
      "batch: 2000 out of 5401, loss: 0.087\n",
      "batch: 3000 out of 5401, loss: 0.148\n",
      "batch: 4000 out of 5401, loss: 0.141\n",
      "batch: 5000 out of 5401, loss: 0.083\n",
      "training, acc: 22.995072819888907, loss: 0.087 (data_loss: 0.086, reg_loss: 0.001, lr: 5.813277525869085e-05\n",
      "validation, accuracy: 22.138478841036694, loss: 0.086, \n",
      "epoch: 3\n",
      "batch: 0 out of 5401, loss: 0.060\n",
      "batch: 1000 out of 5401, loss: 0.118\n",
      "batch: 2000 out of 5401, loss: 0.072\n",
      "batch: 3000 out of 5401, loss: 0.060\n",
      "batch: 4000 out of 5401, loss: 0.099\n",
      "batch: 5000 out of 5401, loss: 0.056\n",
      "training, acc: 22.7297742162029, loss: 0.087 (data_loss: 0.086, reg_loss: 0.001, lr: 4.4241914790072115e-05\n",
      "validation, accuracy: 22.487015850118667, loss: 0.086, \n",
      "epoch: 4\n",
      "batch: 0 out of 5401, loss: 0.227\n",
      "batch: 1000 out of 5401, loss: 0.080\n",
      "batch: 2000 out of 5401, loss: 0.231\n",
      "batch: 3000 out of 5401, loss: 0.135\n",
      "batch: 4000 out of 5401, loss: 0.081\n",
      "batch: 5000 out of 5401, loss: 0.135\n",
      "training, acc: 22.864393413878425, loss: 0.087 (data_loss: 0.086, reg_loss: 0.001, lr: 3.570918440222825e-05\n",
      "validation, accuracy: 22.703266084928547, loss: 0.086, \n",
      "epoch: 5\n",
      "batch: 0 out of 5401, loss: 0.070\n",
      "batch: 1000 out of 5401, loss: 0.045\n",
      "batch: 2000 out of 5401, loss: 0.080\n",
      "batch: 3000 out of 5401, loss: 0.038\n",
      "batch: 4000 out of 5401, loss: 0.184\n",
      "batch: 5000 out of 5401, loss: 0.107\n",
      "training, acc: 23.030882100837186, loss: 0.086 (data_loss: 0.085, reg_loss: 0.001, lr: 2.99356383774884e-05\n",
      "validation, accuracy: 22.46266806598624, loss: 0.086, \n",
      "epoch: 6\n",
      "batch: 0 out of 5401, loss: 0.082\n",
      "batch: 1000 out of 5401, loss: 0.050\n",
      "batch: 2000 out of 5401, loss: 0.058\n",
      "batch: 3000 out of 5401, loss: 0.083\n",
      "batch: 4000 out of 5401, loss: 0.159\n",
      "batch: 5000 out of 5401, loss: 0.046\n",
      "training, acc: 22.970982932894607, loss: 0.086 (data_loss: 0.085, reg_loss: 0.001, lr: 2.5769210946760812e-05\n",
      "validation, accuracy: 22.37589157136815, loss: 0.086, \n",
      "epoch: 7\n",
      "batch: 0 out of 5401, loss: 0.086\n",
      "batch: 1000 out of 5401, loss: 0.078\n",
      "batch: 2000 out of 5401, loss: 0.043\n",
      "batch: 3000 out of 5401, loss: 0.305\n",
      "batch: 4000 out of 5401, loss: 0.062\n",
      "batch: 5000 out of 5401, loss: 0.117\n",
      "training, acc: 22.934941740852373, loss: 0.086 (data_loss: 0.085, reg_loss: 0.001, lr: 2.26208519012826e-05\n",
      "validation, accuracy: 21.833259837729166, loss: 0.087, \n",
      "epoch: 8\n",
      "batch: 0 out of 5401, loss: 0.182\n",
      "batch: 1000 out of 5401, loss: 0.047\n",
      "batch: 2000 out of 5401, loss: 0.224\n",
      "batch: 3000 out of 5401, loss: 0.089\n",
      "batch: 4000 out of 5401, loss: 0.148\n",
      "batch: 5000 out of 5401, loss: 0.176\n",
      "training, acc: 22.71096807064988, loss: 0.086 (data_loss: 0.085, reg_loss: 0.001, lr: 2.0158039025963555e-05\n",
      "validation, accuracy: 22.344450200221253, loss: 0.086, \n",
      "epoch: 9\n",
      "batch: 0 out of 5401, loss: 0.038\n",
      "batch: 1000 out of 5401, loss: 0.043\n",
      "batch: 2000 out of 5401, loss: 0.042\n",
      "batch: 3000 out of 5401, loss: 0.189\n",
      "batch: 4000 out of 5401, loss: 0.043\n",
      "batch: 5000 out of 5401, loss: 0.069\n",
      "training, acc: 22.68721716483259, loss: 0.086 (data_loss: 0.085, reg_loss: 0.001, lr: 1.817884346197895e-05\n",
      "validation, accuracy: 22.410779624637204, loss: 0.086, \n",
      "Model Evaluation on Test Set\n",
      "test set, test acc: 22.303361810386527 test loss: 0.084, \n"
     ]
    }
   ],
   "source": [
    "# Begin training model\n",
    "for epoch in range(10):\n",
    "    print(f'epoch: {epoch}')\n",
    "    accuracy.init(y_train)\n",
    "    np.random.shuffle(keys)\n",
    "    X_train = X_train[keys]\n",
    "    y_train = y_train[keys]\n",
    "\n",
    "    loss_function.new_pass()\n",
    "    accuracy.new_pass()\n",
    "\n",
    "    # Train model in mini batches\n",
    "    for step in range(train_steps):\n",
    "        batch_X = X_train[step * batch_size:(step+1)*batch_size]\n",
    "        batch_y = y_train[step * batch_size:(step+1)*batch_size]\n",
    "\n",
    "        # Forward Pass\n",
    "        dense1.forward(batch_X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dropout1.forward(activation1.output)\n",
    "        dense2.forward(dropout1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "        dropout2.forward(activation2.output)\n",
    "        dense3.forward(dropout2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        # Calculate loss of current batch\n",
    "        data_loss, reg_loss = loss_function.calculate(activation3.output, batch_y, layers, include_regularization=True)\n",
    "        loss = data_loss + reg_loss\n",
    "\n",
    "        # Predicted Prices of current batch\n",
    "        predictions = activation3.output\n",
    "\n",
    "        # Calculate accuracy of current batch\n",
    "        accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "        # Backward Pass\n",
    "        loss_function.backward(activation3.output, batch_y)\n",
    "        activation3.backward(loss_function.dinputs)\n",
    "        dense3.backward(activation3.dinputs)\n",
    "        dropout2.backward(dense3.dinputs)\n",
    "        activation2.backward(dropout2.dinputs)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        dropout1.backward(dense2.dinputs)\n",
    "        activation1.backward(dropout1.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.pre_update_params()\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        # Print loss every 1000 batches\n",
    "        if not step % 1000:\n",
    "            print(f'batch: {step} out of {train_steps}, loss: {loss:.3f}')\n",
    "\n",
    "    # Calculate mean loss after epoch\n",
    "    epoch_data_loss, epoch_reg_loss = \\\n",
    "        loss_function.calculate_accumulated(include_regularization=True)\n",
    "    epoch_loss = epoch_data_loss + epoch_reg_loss\n",
    "\n",
    "    # Calculate mean accuracy after epoch\n",
    "    epoch_accuracy = accuracy.calculate_accumulated()\n",
    "\n",
    "    # Print training progress after epoch\n",
    "    print(f'training, ' +\n",
    "    f'acc: {epoch_accuracy}, ' +\n",
    "    f'loss: {epoch_loss:.3f} (' +\n",
    "    f'data_loss: {epoch_data_loss:.3f}, ' +\n",
    "    f'reg_loss: {epoch_reg_loss:.3f}, ' +\n",
    "    f'lr: {optimizer.current_learning_rate}')\n",
    "\n",
    "    # Reset mean loss before using validation set\n",
    "    loss_function.new_pass()\n",
    "    accuracy.new_pass()\n",
    "\n",
    "    # Pass through validation set for evaluation during training\n",
    "    for step in range(val_steps):\n",
    "\n",
    "        batch_X = X_val[step * batch_size:(step+1)*batch_size]\n",
    "        batch_y = y_val[step * batch_size:(step+1)*batch_size]\n",
    "\n",
    "        dense1.forward(batch_X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "\n",
    "        loss_function.calculate(activation3.output, batch_y, layers)\n",
    "\n",
    "        predictions = activation3.output\n",
    "        accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "    # Calculate mean loss for validation set on current model parameters\n",
    "    val_loss = loss_function.calculate_accumulated()\n",
    "    val_acc = accuracy.calculate_accumulated()\n",
    "\n",
    "    # Print mean validation loss per epoch\n",
    "    print(f'validation, ' +\n",
    "          f'accuracy: {val_acc}, ' +\n",
    "          f'loss: {val_loss:.3f}, ')\n",
    "\n",
    "# Training finished\n",
    "\n",
    "# Test model\n",
    "print(\"Model Evaluation on Test Set\")\n",
    "\n",
    "loss_function.new_pass()\n",
    "accuracy.new_pass()\n",
    "\n",
    "# Pass through testing set\n",
    "for step in range(test_steps):\n",
    "\n",
    "    batch_X = X_test[step * batch_size:(step+1)*batch_size]\n",
    "    batch_y = y_test[step * batch_size:(step+1)*batch_size]\n",
    "\n",
    "    dense1.forward(batch_X)\n",
    "    activation1.forward(dense1.output)\n",
    "    dense2.forward(activation1.output)\n",
    "    activation2.forward(dense2.output)\n",
    "    dense3.forward(activation2.output)\n",
    "    activation3.forward(dense3.output)\n",
    "\n",
    "    loss_function.calculate(activation3.output, batch_y, layers)\n",
    "\n",
    "    predictions = activation3.output\n",
    "    accuracy.calculate(predictions, batch_y)\n",
    "\n",
    "\n",
    "\n",
    "    pairs += [batch_X, batch_y, predictions, accuracy.calculate(predictions, batch_y)]\n",
    "\n",
    "\n",
    "# Calculate mean loss for validation set on current model parameters\n",
    "test_loss = loss_function.calculate_accumulated()\n",
    "test_acc = accuracy.calculate_accumulated()\n",
    "\n",
    "# Print mean validation loss per epoch\n",
    "print(f'test set, ' +\n",
    "      f'test acc: {test_acc}',\n",
    "      f'test loss: {test_loss:.3f}, ')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "outputs": [
    {
     "data": {
      "text/plain": "('x',\n array([1., 0., 0., ..., 0., 0., 0.]),\n 'y',\n 0.23,\n 'pred',\n array([0.33519864]),\n 'acc',\n 0.1424812012089159,\n 'x',\n array([0.9375, 0.    , 0.    , ..., 0.    , 0.    , 0.    ]),\n 'y',\n 0.32,\n 'pred',\n array([0.32947181]),\n 'acc',\n 0.1424812012089159)"
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add accuracy\n",
    "# Make some predictions, if predictions are good, test accuracy on external dataset\n",
    "y_test.tolist(), predictions.tolist()\n",
    "y_test.shape, predictions.shape\n",
    "len(pairs), len(pairs[0]), test_steps\n",
    "\"x\", pairs[0][0], \"y\", pairs[1][0],  \"pred\", pairs[2][0], \"acc\", pairs[3], \"x\", pairs[0][1], \"y\", pairs[1][1],  \"pred\", pairs[2][1], \"acc\", pairs[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "outputs": [
    {
     "data": {
      "text/plain": "'£341590.40'"
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [0.0] * 373\n",
    "#test_batch = [19.0, 0.0, 6.0, 0.012487, 0.533333]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_batch = [0.0, 0.0, 0.0,  0.5797850, 0.546301]\n",
    "\n",
    "test_batch.extend(cols)\n",
    "test_batch[118] = 1.0\n",
    "\n",
    "\n",
    "dense1.forward(test_batch)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "dense3.forward(activation2.output)\n",
    "activation3.forward(dense3.output)\n",
    "\n",
    "\n",
    "\n",
    "predictions = activation3.output\n",
    "predictions = predictions.flatten()\n",
    "price = predictions[0]\n",
    "x = (\"£{:.2f}\".format(price*1000000))\n",
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (feedforward-nn-python)",
   "language": "python",
   "name": "pycharm-41e5e3a3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}